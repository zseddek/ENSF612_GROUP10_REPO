{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Pipeline",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 2532965066523
    },
    "colab": {
      "name": "Clean_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COj57xv6vtY2"
      },
      "source": [
        "## Initialize Cluster "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNUf1vfNvsYo",
        "outputId": "0b687519-d60f-43f0-e6d2-ad1ac23d98e4"
      },
      "source": [
        "# COMPLETED BY: Ajoy\n",
        "\n",
        "import os\n",
        "import atexit\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import pyspark\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import findspark\n",
        "from sparkhpc import sparkjob\n",
        "\n",
        "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
        "def exitHandler(sj,sc):\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Context')\n",
        "        sc.stop()\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Job')\n",
        "        sj.stop()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "#Parameters for the Spark cluster\n",
        "nodes=1\n",
        "tasks_per_node=4 \n",
        "memory_per_task=4096 #4 gig per process, adjust accordingly\n",
        "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
        "# idle so that others may use the resources.\n",
        "walltime=\"1:00\" #1 hours\n",
        "#os.environ['SBATCH_PARTITION']='cpu2019' #Set the appropriate ARC partition\n",
        "\n",
        "sj = sparkjob.sparkjob(\n",
        "     ncores=nodes*tasks_per_node,\n",
        "     cores_per_executor=tasks_per_node,\n",
        "     memory_per_core=memory_per_task,\n",
        "     walltime=walltime\n",
        "    )\n",
        "\n",
        "sj.wait_to_start()\n",
        "time.sleep(60)\n",
        "sc = sj.start_spark()\n",
        "\n",
        "#Register the exit handler                                                                                                     \n",
        "atexit.register(exitHandler,sj,sc)\n",
        "\n",
        "#You need this line if you want to use SparkSQL\n",
        "sqlCtx=SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sparkhpc.sparkjob:Submitted batch job 16059\n",
            "\n",
            "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-3yOyUukMNO"
      },
      "source": [
        "# ENSF 612 Term Project: Extending the README Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9e9c51c9-d9c5-4d85-bb08-e887ec376ece"
        },
        "id": "AtUo0BbFvIaU"
      },
      "source": [
        "#### Loading feature matrix and target vector into PySpark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "11dd3570-dc45-48e7-8bab-664096a9b190"
        },
        "id": "JEJOeIWEvIaR"
      },
      "source": [
        "# COMPLETED BY: Zach Frena\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2e7195cc-57b5-4b62-9aff-a1e700a066d0"
        },
        "id": "U9T46hUQvIaX"
      },
      "source": [
        "# COMPLETED BY: Zach Frena\n",
        "# Original + New Samples (can be found following One Drive Link)\n",
        "FULL_X = \"./FULL_X.csv\"\n",
        "FULL_Y = \"./TARGET_MATRIX_YTRUE_FULL.csv\"\n",
        "\n",
        "## Original Samples (can be found following One Drive Link)\n",
        "# FULL_X = \"./OG_FULL_X.csv\"\n",
        "# FULL_Y = \"./OG_TARGET_MATRIX_YTRUE_FULL.csv\"\n",
        "\n",
        "# # New Samples (can be found following One Drive Link)\n",
        "# FULL_X = \"./NEW_FULL_X.csv\"\n",
        "# FULL_Y = \"./NEW_TARGET_MATRIX_YTRUE_FULL.csv\"\n",
        "\n",
        "file_type = \"csv\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        " \n",
        "spark = sqlCtx\n",
        "\n",
        "X = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\",True) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(FULL_X)\n",
        " \n",
        "Y = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\",True) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(FULL_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select Original Dataset\n"
      ],
      "metadata": {
        "id": "0maR1BHjf4a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Zach Frena\n",
        "print(\"Originial Samples\\n\\tX:\",(X.count(), len(X.columns)))\n",
        "print(\"\\tY:\",(Y.count(), len(Y.columns)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDzF39nlhhX3",
        "outputId": "42fbde6c-aac0-4587-a02f-76c2186d9f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Originial Samples\n",
            "\tX: (666, 13350)\n",
            "\tY: (666, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Zach Frena\n",
        "print(\"Originial Samples\\n\\tX:\",(X.count(), len(X.columns)))\n",
        "print(\"\\tY:\",(Y.count(), len(Y.columns)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3QZsyti50h",
        "outputId": "4e08b4d2-f33a-4e92-8410-2899d9195af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Originial Samples\n",
            "\tX: (666, 13350)\n",
            "\tY: (666, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing for ML Pipeline via StopWord Removal and Column Dropping"
      ],
      "metadata": {
        "id": "4OWOiJuqH1QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Zach Frena\n",
        "print(\"Number of features before preprocessing:\", len(X.columns))"
      ],
      "metadata": {
        "id": "ktxWbpOSJL71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cae1f0e-b1ce-433f-803f-5f59e1c78ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features before preprocessing: 13350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Ziryan Seddeck \n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_en = stopwords.words('english')\n",
        "my_stop_words = ['!', '.', ',', '?', '\\\\', '/', ':', 'n', 'I\\'m',\n",
        "                '[', ']', '(', ')', '{', '}', '_']\n",
        "stop_en += my_stop_words\n",
        "stop_en"
      ],
      "metadata": {
        "id": "LHPxIDgJHTkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02941a08-1fb6-4346-a000-b6073234acd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages (3.3)\r\n",
            "Requirement already satisfied: six in /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 21.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/tobi.odufeso/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " '!',\n",
              " '.',\n",
              " ',',\n",
              " '?',\n",
              " '\\\\',\n",
              " '/',\n",
              " ':',\n",
              " 'n',\n",
              " \"I'm\",\n",
              " '[',\n",
              " ']',\n",
              " '(',\n",
              " ')',\n",
              " '{',\n",
              " '}',\n",
              " '_']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Removing features with \"_\" present"
      ],
      "metadata": {
        "id": "Y49caMibIhll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Ziryan Seddeck \n",
        "sc = spark.sparkSession.sparkContext\n",
        "columns_rdd = sc.parallelize(X.columns)\n",
        "words_no_dash = columns_rdd.filter(lambda word: '_' not in word)"
      ],
      "metadata": {
        "id": "BcHZi84gIgD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Removing Stop Words"
      ],
      "metadata": {
        "id": "VCn1bkX5PSVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Ziryan Seddeck \n",
        "\n",
        "final_columns = words_no_dash.filter(lambda word: word.lower() not in stop_en).collect()"
      ],
      "metadata": {
        "id": "nykmgMHnNQQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Ziryan Seddeck \n",
        "print(len(final_columns))"
      ],
      "metadata": {
        "id": "DDFf2utGP_RA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1086c897-465a-4cfe-a066-15b164090c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Updating Dataframe Columns to reflect final_columns list\n"
      ],
      "metadata": {
        "id": "Vzk12KkoIgzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Ziryan Seddeck \n",
        "X = X.select(final_columns)"
      ],
      "metadata": {
        "id": "tg2qPAhPUCpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Ziryan Seddeck \n",
        "len(X.columns)"
      ],
      "metadata": {
        "id": "BNRF4mPVUY-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb7c63f-6fe1-4769-ee25-d5771ce4bf90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11969"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "27cc1bb3-9f66-4b01-b8b2-6ac4be173421"
        },
        "id": "_NnVsqGJvIab"
      },
      "source": [
        "## Machine Learning Classification Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Tobi Odufeso\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.tuning import TrainValidationSplitModel\n",
        "\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "AgHRyLfibUSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Zach Frena\n",
        "# COMPLETED BY: Tobi Odufeso\n",
        "# COMPLETED BY: Ziryan Seddeck  \n",
        "\n",
        "def trainModel(traning_data, testing_data, name): \n",
        "  label_name = str(name)\n",
        "  formatter = \"{0:10.4f}\"\n",
        "  # Pipeline\n",
        "  print(\"\\nGrid Start\")\n",
        "  SVC = LinearSVC()\n",
        "  grid = ParamGridBuilder().addGrid(SVC.regParam, [0.00001, 0.0001, 0.001]).addGrid(SVC.threshold, [0.0, 0.5, 1]).build()\n",
        "  evaluator = MulticlassClassificationEvaluator()\n",
        "  tvs = TrainValidationSplit(estimator=SVC, estimatorParamMaps=grid, evaluator=evaluator, trainRatio=0.75, parallelism=24, seed=42) \n",
        "  print(\"Grid End\")\n",
        "  \n",
        "  print(\"\\nTraining Start\")\n",
        "  trained_model = tvs.fit(traning_data)\n",
        "  print(\"Training Scores (F1):\",trained_model.validationMetrics)\n",
        "  print(\"Trining End\")\n",
        "\n",
        "  # Training Scores \n",
        "  print(\"\\nStart Writting Training Score (F1) to Text File.\")\n",
        "  textfile = open(\"models_metrics.txt\", \"a\") \n",
        "  textfile.write(\"AVERAGE TRAINING SCORES(F1) FOR '\"+label_name+\"': \"+str(trained_model.validationMetrics)+\"\\n\")\n",
        "  for model in list(zip(trained_model.validationMetrics, grid)):\n",
        "      textfile.write(str(model)+\"\\n\")\n",
        "  \n",
        "  bestModel = trained_model.bestModel\n",
        "  message = \"The best value for the regularization constant(C) is: \"+str(bestModel._java_obj.getRegParam())\n",
        "  print(\"\\n\"+message)\n",
        "  textfile.write(message+\"\\n\\n\")\n",
        "  textfile.close()\n",
        "  print(\"Write Complete\")\n",
        "\n",
        "  # Testing Model\n",
        "  print(\"\\nTransform Start\")\n",
        "  y_pred = trained_model.transform(testing_data)\n",
        "  print(\"Transform End\")\n",
        "  # y_pred_data = trained_model.transform(testing_data).select(\"label\", \"prediction\")  \n",
        "  y_pred_data = y_pred.select(\"label\", \"prediction\")  \n",
        "    \n",
        "  print(\"\\nWritting Predicted Labels to File\")\n",
        "  y_pred_data.write.csv(label_name) # Create directory with same name as the target label \n",
        "  print(\"Writting Complete\")\n",
        "\n",
        "  print(\"\\nWritting F1 Score to File\")\n",
        "  evaluator = MulticlassClassificationEvaluator()\n",
        "  trained_scores_file = open(\"trained_scores.txt\", \"a\") \n",
        "  trained_scores_file.write(\"WEIGHTED AVERAGE SCORE FOR '\"+label_name+\"': \"+str(evaluator.evaluate(y_pred_data)) +\"\\n\")\n",
        "  print(\"Test Score(F1):\", formatter.format(evaluator.evaluate(y_pred_data)))\n",
        "  print(\"Writting Complete\")\n",
        "\n",
        "  # Create Confusion Matrix and Binary Classification Report \n",
        "  print(\"\\nWritting Confusion Matrix and Binary Classification Report to File\")\n",
        "\n",
        "  y_true = y_pred_data.select(\"label\").collect()\n",
        "  y_pred = y_pred_data.select(\"prediction\").collect()\n",
        "\n",
        "  crp = classification_report(y_true, y_pred, output_dict=True)\n",
        "  class_report = pd.DataFrame(crp)\n",
        "  cfm = np.array2string(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "  cr_textfile = open(\"classification_report.csv\", \"a\") \n",
        "  cr_textfile.write(\"CONFUSION MATRIX FOR '\"+label_name+\"' \\n\")\n",
        "  cr_textfile.write(\"\\t\"+cfm+\"\\n\\n\")\n",
        "  cr_textfile.close()\n",
        "\n",
        "  cr_textfile = open(\"classification_report.csv\", \"a\") \n",
        "  cr_textfile.write(\"CLASSIFICATION REPORT FOR '\"+label_name+\"' \\n\")\n",
        "  cr_textfile.close()\n",
        "  class_report.to_csv(\"classification_report.csv\", mode=\"a\", header=True)\n",
        "\n",
        "  # Formating \n",
        "  cr_textfile = open(\"classification_report.csv\", \"a\") \n",
        "  cr_textfile.write(\"\\n\\n\\n\")\n",
        "  cr_textfile.close()\n",
        "  print(\"Writting Complete\")\n"
      ],
      "metadata": {
        "id": "9ENHbjBX8LtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Prediction using Optimized SVC ML Classifier\n"
      ],
      "metadata": {
        "id": "s5nB1dELJhgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETED BY: Tobi Odufeso\n",
        " \n",
        "y_name = 8  # SELECT COLUMN IN TARGET MATRIX (make sure to reload data before running)\n",
        "print(\"Converting Target Matrix to Selected Vector\")\n",
        "y =  Y.select(str(y_name)) \n",
        "label_name = y.columns[0]\n",
        "\n",
        "# Convert features into vector \n",
        "assembler = VectorAssembler(inputCols=X.columns, outputCol=\"features_\", handleInvalid=\"keep\")\n",
        "X = assembler.transform(X)\n",
        "y_label = y.withColumnRenamed(label_name, \"label_\")\n",
        "y = y_label.withColumn('rowIdx', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
        "X = X.withColumn('rowIdx', row_number().over(Window.orderBy(monotonically_increasing_id()))) \n",
        "features_with_label = (X.join(y, on=[\"rowIdx\"]).drop(\"rowIdx\")).select(\"features_\", \"label_\").withColumnRenamed(\"features_\", \"features\").withColumnRenamed(\"label_\", \"label\")\n",
        "\n",
        "training_data, testing_data = features_with_label.randomSplit([0.9,0.1], seed=42)\n",
        "\n",
        "trainModel(training_data, testing_data, y_name)"
      ],
      "metadata": {
        "id": "g2PeX1pD_QqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d891cb25-5a35-46b2-a93b-104356d5a9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting Target Matrix to Selected Vector\n",
            "\n",
            "Grid Start\n",
            "Grid End\n",
            "\n",
            "Training Start\n",
            "Training Scores (F1): [0.9801935479262307, 0.9808179077732465, 0.9814550464691736, 0.9801935479262307, 0.9808179077732465, 0.9814550464691736, 0.9801935479262307, 0.9808179077732465, 0.9814550464691736]\n",
            "Trining End\n",
            "\n",
            "Start Writting Training Score (F1) to Text File.\n",
            "\n",
            "The best value for the regularization constant(C) is: 1e-05\n",
            "Write Complete\n",
            "\n",
            "Transform Start\n",
            "Transform End\n",
            "\n",
            "Writting Predicted Labels to File\n",
            "Writting Complete\n",
            "\n",
            "Writting F1 Score to File\n",
            "Test Score(F1):     0.9759\n",
            "Writting Complete\n",
            "\n",
            "Writting Confusion Matrix and Binary Classification Report to File\n",
            "Writting Complete\n"
          ]
        }
      ]
    }
  ]
}