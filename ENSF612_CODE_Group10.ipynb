{"cells":[{"cell_type":"markdown","source":["## Initialize TALC Cluster"],"metadata":{"id":"COj57xv6vtY2","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef8715c5-5da9-44b1-bba4-ba75ea8f416c"}}},{"cell_type":"code","source":["# COMPLETED BY: Ajoy\n\nimport os\nimport atexit\nimport sys\nimport time\n\nimport pyspark\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SQLContext\nimport findspark\nfrom sparkhpc import sparkjob\n\n#Exit handler to clean up the Spark cluster if the script exits or crashes\ndef exitHandler(sj,sc):\n    try:\n        print('Trapped Exit cleaning up Spark Context')\n        sc.stop()\n    except:\n        pass\n    try:\n        print('Trapped Exit cleaning up Spark Job')\n        sj.stop()\n    except:\n        pass\n\nfindspark.init()\n\n#Parameters for the Spark cluster\nnodes=1\ntasks_per_node=4 \nmemory_per_task=4096 #4 gig per process, adjust accordingly\n# Please estimate walltime carefully to keep unused Spark clusters from sitting \n# idle so that others may use the resources.\nwalltime=\"1:00\" #1 hours\n#os.environ['SBATCH_PARTITION']='cpu2019' #Set the appropriate ARC partition\n\nsj = sparkjob.sparkjob(\n     ncores=nodes*tasks_per_node,\n     cores_per_executor=tasks_per_node,\n     memory_per_core=memory_per_task,\n     walltime=walltime\n    )\n\nsj.wait_to_start()\ntime.sleep(60)\nsc = sj.start_spark()\n\n#Register the exit handler                                                                                                     \natexit.register(exitHandler,sj,sc)\n\n#You need this line if you want to use SparkSQL\nsqlCtx=SQLContext(sc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNUf1vfNvsYo","outputId":"0b687519-d60f-43f0-e6d2-ad1ac23d98e4","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc1a8ab7-7e5d-4925-ba1f-a13fd2320754"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"INFO:sparkhpc.sparkjob:Submitted batch job 16059\n\nINFO:sparkhpc.sparkjob:Submitted cluster 0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["INFO:sparkhpc.sparkjob:Submitted batch job 16059\n\nINFO:sparkhpc.sparkjob:Submitted cluster 0\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# ENSF 612 Term Project: Extending the README Classifier"],"metadata":{"id":"B-3yOyUukMNO","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbaae860-f1b9-466f-a198-7d13a3442f55"}}},{"cell_type":"markdown","source":["#### Loading feature matrix and target vector into PySpark dataframe"],"metadata":{"id":"AtUo0BbFvIaU","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e9c51c9-d9c5-4d85-bb08-e887ec376ece"}}},{"cell_type":"code","source":["# COMPLETED BY: Zach Frena\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *"],"metadata":{"id":"JEJOeIWEvIaR","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11dd3570-dc45-48e7-8bab-664096a9b190"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# COMPLETED BY: Zach Frena\n# Original + New Samples (can be found following One Drive Link)\nFULL_X = \"./FULL_X.csv\"\nFULL_Y = \"./TARGET_MATRIX_YTRUE_FULL.csv\"\n\n## Original Samples (can be found following One Drive Link)\n# FULL_X = \"./OG_FULL_X.csv\"\n# FULL_Y = \"./OG_TARGET_MATRIX_YTRUE_FULL.csv\"\n\n# # New Samples (can be found following One Drive Link)\n# FULL_X = \"./NEW_FULL_X.csv\"\n# FULL_Y = \"./NEW_TARGET_MATRIX_YTRUE_FULL.csv\"\n\nfile_type = \"csv\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n \nspark = sqlCtx\n\nX = spark.read.format(file_type) \\\n  .option(\"inferSchema\",True) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(FULL_X)\n \nY = spark.read.format(file_type) \\\n  .option(\"inferSchema\",True) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(FULL_Y)"],"metadata":{"id":"U9T46hUQvIaX","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e7195cc-57b5-4b62-9aff-a1e700a066d0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Select Input Dataset\nThis could be either the old + new (considered the main), old, or new datasets as specifed above in the comments."],"metadata":{"id":"0maR1BHjf4a5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a5a8827-e927-4e75-aa15-62a8c0836dea"}}},{"cell_type":"code","source":["# COMPLETED BY: Zach Frena\nprint(\"Originial + New Samples\\n\\tX:\",(X.count(), len(X.columns)))\nprint(\"\\tY:\",(Y.count(), len(Y.columns)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDzF39nlhhX3","outputId":"42fbde6c-aac0-4587-a02f-76c2186d9f4a","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e448cdda-7103-4112-b276-23a907792c60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Originial + New Samples\n\tX: (4331, 13350)\n\tY: (4331, 8)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Originial + New Samples\n\tX: (4331, 13350)\n\tY: (4331, 8)\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Preprocessing for ML Pipeline via StopWord Removal and Column Dropping"],"metadata":{"id":"4OWOiJuqH1QA","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0273eb9b-035f-4721-b5f8-a7f9c45ee547"}}},{"cell_type":"code","source":["# COMPLETED BY: Zach Frena\nprint(\"Number of features before preprocessing:\", len(X.columns))"],"metadata":{"id":"ktxWbpOSJL71","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9cae1f0e-b1ce-433f-803f-5f59e1c78ff6","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5920c1f-3f42-40b3-9a1c-c2ca701aebbc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of features before preprocessing: 13350\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of features before preprocessing: 13350\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# COMPLETED BY: Ziryan Seddek\n!pip install nltk\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_en = stopwords.words('english')\nmy_stop_words = ['!', '.', ',', '?', '\\\\', '/', ':', 'n', 'I\\'m',\n                '[', ']', '(', ')', '{', '}', '_']\nstop_en += my_stop_words\nstop_en"],"metadata":{"id":"LHPxIDgJHTkQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"02941a08-1fb6-4346-a000-b6073234acd7","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bb20d20-a21f-460d-9d28-991744069bb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nOut[20]: ['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\",\n '!',\n '.',\n ',',\n '?',\n '\\\\',\n '/',\n ':',\n 'n',\n \"I'm\",\n '[',\n ']',\n '(',\n ')',\n '{',\n '}',\n '_']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nOut[20]: ['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\",\n '!',\n '.',\n ',',\n '?',\n '\\\\',\n '/',\n ':',\n 'n',\n \"I'm\",\n '[',\n ']',\n '(',\n ')',\n '{',\n '}',\n '_']"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["1. Removing features with \"_\" present"],"metadata":{"id":"Y49caMibIhll","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16a8a674-b957-48c5-aa48-396934404f1b"}}},{"cell_type":"code","source":["# COMPLETED BY: Ziryan Seddek \nsc = spark.sparkSession.sparkContext\ncolumns_rdd = sc.parallelize(X.columns)\nwords_no_dash = columns_rdd.filter(lambda word: '_' not in word)"],"metadata":{"id":"BcHZi84gIgD7","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d36797c7-cb1a-4c29-b701-b3ee8cb88be8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["2. Removing Stop Words"],"metadata":{"id":"VCn1bkX5PSVi","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bffc2eec-ba4d-478c-a536-c09a6030f575"}}},{"cell_type":"code","source":["# COMPLETED BY: Ziryan Seddek\n\nfinal_columns = words_no_dash.filter(lambda word: word.lower() not in stop_en).collect()"],"metadata":{"id":"nykmgMHnNQQ5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f1121ca-a684-4119-ae3e-ee03bed9ee76"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# COMPLETED BY: Ziryan Seddek\nprint(len(final_columns))"],"metadata":{"id":"DDFf2utGP_RA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1086c897-465a-4cfe-a066-15b164090c2d","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17f973b7-b952-491f-b52d-b7e8bfa8285f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"11969\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["11969\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["3. Updating Dataframe Columns to reflect final_columns list"],"metadata":{"id":"Vzk12KkoIgzr","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6dceec3-ccb9-4aa8-8eb2-0fbe500f2e67"}}},{"cell_type":"code","source":["# COMPLETED BY: Ziryan Seddek\nX = X.select(final_columns)"],"metadata":{"id":"tg2qPAhPUCpE","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a1ce068-a58b-439c-8159-0191e6d20c57"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# COMPLETED BY: Ziryan Seddeck \nlen(X.columns)"],"metadata":{"id":"BNRF4mPVUY-U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cb7c63f-6fe1-4769-ee25-d5771ce4bf90","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfab3187-1edd-4251-881a-2c5974f7125c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: 11969","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: 11969"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Machine Learning Classification Pipeline"],"metadata":{"id":"_NnVsqGJvIab","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27cc1bb3-9f66-4b01-b8b2-6ac4be173421"}}},{"cell_type":"code","source":["# COMPLETED BY: Tobi Odufeso\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import monotonically_increasing_id, row_number\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as F\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LinearSVC\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\nfrom pyspark.ml.tuning import TrainValidationSplitModel\n\nimport sklearn\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix"],"metadata":{"id":"AgHRyLfibUSV","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f5d7e83-64d4-41a4-8f90-07084cc4c9e1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# COMPLETED BY: Zach Frena\n# COMPLETED BY: Tobi Odufeso\n# COMPLETED BY: Ziryan Seddek\n\ndef trainModel(traning_data, testing_data, name): \n  label_name = str(name)\n  formatter = \"{0:10.4f}\"\n  # Pipeline\n  print(\"\\nGrid Start\")\n  SVC = LinearSVC()\n  grid = ParamGridBuilder().addGrid(SVC.regParam, [0.00001, 0.0001, 0.001]).addGrid(SVC.threshold, [0.0, 0.5, 1]).build()\n  evaluator = MulticlassClassificationEvaluator()\n  tvs = TrainValidationSplit(estimator=SVC, estimatorParamMaps=grid, evaluator=evaluator, trainRatio=0.75, parallelism=24, seed=42) \n  print(\"Grid End\")\n  \n  print(\"\\nTraining Start\")\n  trained_model = tvs.fit(traning_data)\n  print(\"Training Scores (F1):\",trained_model.validationMetrics)\n  print(\"Trining End\")\n\n  # Training Scores \n  print(\"\\nStart Writting Training Score (F1) to Text File.\")\n  textfile = open(\"models_metrics.txt\", \"a\") \n  textfile.write(\"AVERAGE TRAINING SCORES(F1) FOR '\"+label_name+\"': \"+str(trained_model.validationMetrics)+\"\\n\")\n  for model in list(zip(trained_model.validationMetrics, grid)):\n      textfile.write(str(model)+\"\\n\")\n  \n  bestModel = trained_model.bestModel\n  message = \"The best value for the regularization constant(C) is: \"+str(bestModel._java_obj.getRegParam())\n  print(\"\\n\"+message)\n  textfile.write(message+\"\\n\\n\")\n  textfile.close()\n  print(\"Write Complete\")\n\n  # Testing Model\n  print(\"\\nTransform Start\")\n  y_pred = trained_model.transform(testing_data)\n  print(\"Transform End\")\n  # y_pred_data = trained_model.transform(testing_data).select(\"label\", \"prediction\")  \n  y_pred_data = y_pred.select(\"label\", \"prediction\")  \n    \n  print(\"\\nWritting Predicted Labels to File\")\n  y_pred_data.write.csv(label_name) # Create directory with same name as the target label \n  print(\"Writting Complete\")\n\n  print(\"\\nWritting F1 Score to File\")\n  evaluator = MulticlassClassificationEvaluator()\n  trained_scores_file = open(\"trained_scores.txt\", \"a\") \n  trained_scores_file.write(\"WEIGHTED AVERAGE SCORE FOR '\"+label_name+\"': \"+str(evaluator.evaluate(y_pred_data)) +\"\\n\")\n  print(\"Test Score(F1):\", formatter.format(evaluator.evaluate(y_pred_data)))\n  print(\"Writting Complete\")\n\n  # Create Confusion Matrix and Binary Classification Report \n  print(\"\\nWritting Confusion Matrix and Binary Classification Report to File\")\n\n  y_true = y_pred_data.select(\"label\").collect()\n  y_pred = y_pred_data.select(\"prediction\").collect()\n\n  crp = classification_report(y_true, y_pred, output_dict=True)\n  class_report = pd.DataFrame(crp)\n  cfm = np.array2string(confusion_matrix(y_true, y_pred))\n\n  cr_textfile = open(\"classification_report.csv\", \"a\") \n  cr_textfile.write(\"CONFUSION MATRIX FOR '\"+label_name+\"' \\n\")\n  cr_textfile.write(\"\\t\"+cfm+\"\\n\\n\")\n  cr_textfile.close()\n\n  cr_textfile = open(\"classification_report.csv\", \"a\") \n  cr_textfile.write(\"CLASSIFICATION REPORT FOR '\"+label_name+\"' \\n\")\n  cr_textfile.close()\n  class_report.to_csv(\"classification_report.csv\", mode=\"a\", header=True)\n\n  # Formating \n  cr_textfile = open(\"classification_report.csv\", \"a\") \n  cr_textfile.write(\"\\n\\n\\n\")\n  cr_textfile.close()\n  print(\"Writting Complete\")\n"],"metadata":{"id":"9ENHbjBX8LtB","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f517f8a-11f2-4193-b75e-b751667e54ac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Training and Prediction using Optimized SVC ML Classifier"],"metadata":{"id":"s5nB1dELJhgd","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e209faab-5ec0-4199-9b5f-a62f020569de"}}},{"cell_type":"code","source":["# COMPLETED BY: Tobi Odufeso\n \ny_name = 8  # SELECT COLUMN IN TARGET MATRIX (make sure to reload data before running)\nprint(\"Converting Target Matrix to Selected Vector\")\ny =  Y.select(str(y_name)) \nlabel_name = y.columns[0]\n\n# Convert features into vector \nassembler = VectorAssembler(inputCols=X.columns, outputCol=\"features_\", handleInvalid=\"keep\")\nX = assembler.transform(X)\ny_label = y.withColumnRenamed(label_name, \"label_\")\ny = y_label.withColumn('rowIdx', row_number().over(Window.orderBy(monotonically_increasing_id())))\nX = X.withColumn('rowIdx', row_number().over(Window.orderBy(monotonically_increasing_id()))) \nfeatures_with_label = (X.join(y, on=[\"rowIdx\"]).drop(\"rowIdx\")).select(\"features_\", \"label_\").withColumnRenamed(\"features_\", \"features\").withColumnRenamed(\"label_\", \"label\")\n\ntraining_data, testing_data = features_with_label.randomSplit([0.9,0.1], seed=42)\n\ntrainModel(training_data, testing_data, y_name)"],"metadata":{"id":"g2PeX1pD_QqU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d891cb25-5a35-46b2-a93b-104356d5a9e7","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc56e345-4da8-4152-843b-915ed483a72a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Converting Target Matrix to Selected Vector\n\nGrid Start\nGrid End\n\nTraining Start\nTraining Scores (F1): [0.9801935479262307, 0.9808179077732465, 0.9814550464691736, 0.9801935479262307, 0.9808179077732465, 0.9814550464691736, 0.9801935479262307, 0.9808179077732465, 0.9814550464691736]\nTrining End\n\nStart Writting Training Score (F1) to Text File.\n\nThe best value for the regularization constant(C) is: 1e-05\nWrite Complete\n\nTransform Start\nTransform End\n\nWritting Predicted Labels to File\nWritting Complete\n\nWritting F1 Score to File\nTest Score(F1):     0.9759\nWritting Complete\n\nWritting Confusion Matrix and Binary Classification Report to File\nWritting Complete\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Converting Target Matrix to Selected Vector\n\nGrid Start\nGrid End\n\nTraining Start\nTraining Scores (F1): [0.9801935479262307, 0.9808179077732465, 0.9814550464691736, 0.9801935479262307, 0.9808179077732465, 0.9814550464691736, 0.9801935479262307, 0.9808179077732465, 0.9814550464691736]\nTrining End\n\nStart Writting Training Score (F1) to Text File.\n\nThe best value for the regularization constant(C) is: 1e-05\nWrite Complete\n\nTransform Start\nTransform End\n\nWritting Predicted Labels to File\nWritting Complete\n\nWritting F1 Score to File\nTest Score(F1):     0.9759\nWritting Complete\n\nWritting Confusion Matrix and Binary Classification Report to File\nWritting Complete\n"]},"transient":null}],"execution_count":0}],"metadata":{"colab":{"name":"Clean_Pipeline.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"application/vnd.databricks.v1+notebook":{"notebookName":"ENSF612_CODE_Group10","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3271495152886038}},"nbformat":4,"nbformat_minor":0}
